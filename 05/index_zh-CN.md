# LLM 应用

[English](./index.md) | 简体中文

自从引入了Transformer架构（[详见此处](../04/index_zh-CN.md)）以来，大语言模型取得了巨大的进步。由于自然语言是序列信息，它非常适合基于Transformer架构的神经网络训练。其核心原理是，模型接受一个按有意义顺序排列的标记列表（可以是单词、标点符号或短语等），试图预测下一个标记，这是通过对输入标记应用权重计算得出的输出。

例如，如果你给训练好的LLM模型输入"我爱"，它会尝试从整个词汇表（通常包含数万个标记）中预测下一个标记。它可能会给"你"分配0.35的概率，给"运动"分配0.15的概率，给"音乐"分配0.08的概率，给"阅读"分配0.05的概率，并给其他数千个可能的标记分配较小的概率。LLM应用通常选择概率最高的标记（在这种情况下是"你"）作为输出。为了继续，LLM应用将"我爱你"作为新输入，预测下一个标记（很可能是"。"，概率很高），从而产生句子"我爱你。"。

你可能会问，如果我们使用这样的循环来查询模型，它怎么知道什么时候应该停止？诀窍在于训练数据。我们不只是向模型提供纯文本。我们在文本中放入特殊标记，如"\<\|start\|>"和"\<\|end\|>"来表示文本块的开始和结束，这些也算作标记。模型学会输出这种类型的标记，然后LLM应用在看到这种标记时就会中断循环。因此，在ChatGPT、Claude等LLM应用中，你输入一些查询（我们称之为提示），模型会一个词一个词地输出。在用户界面中，它看起来像一个人在试着与你交谈。

现在，让我们深入技术细节。

在训练过程中，第一步叫做标记化。对于文本输入，我们必须将信息转换为模型能够理解的东西，即数字，因为计算机的基础是数学计算。对于一个模型，它维护一个标记列表，将文本映射到数字。当提供文本块时，标记器将其分解为标记序列，然后显示供模型训练的数字列表。当模型输出时，它也输出数字列表的概率，然后我们使用标记器将数字解码为标记以便再次显示。

例如，我们有这样一个标记映射列表：

| 标记 | ID |
|------|-----|
| <\|start\|> | 1 |
| 我 | 2 |
| 爱 | 3 |
| 你 | 4 |
| 运动 | 5 |
| 音乐 | 6 |
| . | 7 |
| <\|end\|> | 8 |

当训练模型时，我们输入"我爱"，然后模型实际看到`[1,2,3]`，经过计算后，它为所有可能的下一个标记输出概率。例如，它可能为标记4（"你"）输出0.5，为标记5（"运动"）输出0.3，为标记6（"音乐"）输出0.2，然后我们选择概率最高的答案4，即标记"你"。

实际的训练分为以下步骤：

**预训练**
在这一步中，LLM学习文本的内在联系，即自然语言序列。模型训练者从互联网、书籍、人类作家等来源获取大量文本块。然后他们清理并将其格式化为我们上面讨论的结构。对于像"我爱你。"这样的块，基于Transformer的模型使用因果语言建模进行训练：模型看到标记在序列中的某个位置，并尝试预测下一个标记。例如，给定"我爱"，模型应该预测"你"，给定"我爱你"，它应该预测"。"。模型计算所有可能下一个标记的概率，然后衡量预测的概率分布与实际下一个标记的匹配程度。通过反向传播，模型调整其权重以最小化这种损失。这个过程在大量文本数据上重复进行，直到模型收敛到可接受的性能。通过数十亿个参数，模型学习标记之间的复杂模式和关系，使其能够生成连贯的文本。由于模型输出概率分布，它可以对相同的输入产生不同的输出。

**监督微调**
预训练后，模型已经学会了一般的语言模式，但可能无法很好地遵循指令或产生有用的响应。在监督微调（SFT）中，模型在精心策划的高质量问答对、对话和指令遵循示例数据集上进行训练。这教会模型适当地响应用户查询并遵循特定格式。训练过程类似于预训练，但使用更小、更高质量的数据集，专注于期望的行为。

例如，对于聊天模型，模型需要学习什么是用户输入，应该如何回答该输入。作为训练者，会准备大量查询，然后雇佣人类标注员为这些查询创建高质量的答案。对话对将被格式化成这样：

```plain
<|im_start|>user<|im_sep|>什么是西瓜？<|im_end|><|im_start|>assistant<|im_sep|>它是一种多汁的水果，外绿内红<|im_end|>
```

然后通过这种类型的数据，模型学会区分问题并预测适当的答案。有了合适的用户界面，LLM应用就能像人类一样与你交谈。对于预训练阶段，可能需要数月时间和数百万美元的计算能力，但是对于SFT，通常只需要几天到几周的时间，使用适当的数据和显著更少的计算资源。总结来说，训练后的模型是人类标注员提供数据的有损模仿。

**强化学习**
最后一步通常涉及基于人类反馈的强化学习（RLHF）。一般来说，让人类创建所有内容是一项艰难的任务，但让人类标注员比较模型的哪个输出更好要容易得多。因此，在RLHF步骤中，我们要求模型为相同的输入生成许多次输出，然后要求人类标注员对质量、有用性和安全性进行排名。这些偏好提供奖励来使用强化学习算法（如PPO（近端策略优化，一种通过在可信区域内进行多个小的更新步骤来优化策略的算法，以避免性能崩溃同时最大化奖励））训练语言模型。这个过程通常用于帮助将模型的输出与人类价值观和偏好保持一致，使其更有用、更无害、更诚实。

这一步已经发展为用于训练模型能够推理。回想起来，模型一次只预测一个标记，所以如果我们给模型一个难题并直接要求答案，我们是在要求模型一次性计算出答案。但是，如果我们要求模型解释得出答案的详细原因，那么模型会按顺序逐步生成推理标记，这为后续标记预测提供了更多上下文，并允许模型更有效地利用其学习到的模式，使答案更可能准确。

DeepSeek公司在其推理模型`deepseek-R1`中使用了这种类型的训练。在其RLHF阶段，训练者提供了带有明确推理步骤的数据，然后根据人类对推理过程和最终答案的偏好来奖励模型：

```plain
<|im_start|>user<|im_sep|>什么是西瓜？<|im_end|><|im_start|>assistant<|im_sep|><think>用户在询问西瓜。我应该提供一个清晰、事实性的描述，涵盖关键特征...</think>它是一种大型圆形水果，外皮绿色，内部果肉红色，通常在夏天食用。<|im_end|>
```

在他们的论文中，研究人员发现了一个"顿悟时刻"，模型找到了重新生成更好答案的点，因为整个思考过程为预测下一个标记提供了更多上下文。这大大改善了模型的推理性能，从而产生了更准确的答案。

RLHF过程的很多部分仍在实验中。理想情况下，偏好应该始终通过人类处理模型的大量生成来提供。这在工业上是昂贵的，所以一些模型训练者开始使用AI反馈（AIF）——训练单独的AI模型基于学习到的人类偏好来评估输出，然后使用这些AI评估器来提供反馈，而不是人类评分员。然而，这个过程是有损的，所以它引入了新问题，比如奖励模型学会作弊，然后产生糟糕的答案。整个行业仍在努力改进RLHF，也许在未来这一步会被更好的解决方案取代。
