[English](./index.md) | 简体中文

# 代码先行

[参考源代码](https://github.com/karpathy/llm.c)

大语言模型通常采用以下训练步骤：

1. **定义模型架构**：指定神经网络的类型（例如，transformer），层数，每层的大小，以及学习率、批大小等超参数。
2. **准备和预处理数据**：收集大规模数据集，进行预处理，包括分词、去噪音，并将数据转换为适合模型处理的格式。这一步通常不需要明确标注输入输出对，因为 LLM 通常使用无监督或自监督学习。
3. **初始化模型**：为模型中的权重和偏置设置初始值，通常使用 Xavier 初始化或 He 初始化等方法。
4. **前向传递**：将输入数据传递给模型以生成预测结果。模型逐层处理输入数据，生成输出。
5. **计算损失**：使用损失函数（例如分类任务中的交叉熵损失）将模型输出与目标输出进行比较。损失量化模型预测与实际目标之间的差距。
6. **反向传递（反向传播）**：通过应用链式法则，计算损失相对于每个模型参数（权重和偏置）的梯度。这一步将误差向网络的反向传播。
7. **更新权重**：使用优化算法（例如随机梯度下降，Adam）调整模型的权重，以最小化损失。优化器根据计算的梯度更新权重。
8. **遍历数据集**：对数据集进行多次迭代（训练轮次），重复前向传递、损失计算、反向传递和权重更新。这包括将数据分批传递给模型，不断调整权重。
9. **验证和微调**：定期在单独的验证数据集上评估模型性能，监控其表现以避免过拟合。微调超参数并进行必要的调整。
10. **模型准备就绪**：一旦训练收敛且模型在训练和验证数据集上表现良好，即可认为模型准备好部署。模型还可以进一步在特定任务或数据集上进行微调。
